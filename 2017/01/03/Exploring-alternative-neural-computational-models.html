<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <title>Exploring alternative neural computational models.</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Exploring alternative neural computational models.">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Exploring alternative neural computational models.">
    <meta property="og:description" content="">

    <!-- <meta name="twitter:site" content="">

<meta name="twitter:creator" content="">

<meta name="google-site-verification" content="">

<meta property="fb:admins" content="">
 -->

    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="/apple-touch-icon-precomposed.png" rel="apple-touch-icon">

    <link href="//fonts.googleapis.com/" rel="dns-prefetch">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Open+Sans:700,400&subset=latin,latin-ext" rel="stylesheet">

    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/main.min.css?v=1483642327254"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/custom.css?v=1483642327254"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/asciidoctor-foundation.css?v=1483642327254"/>




    <script type="text/javascript">
      var ga_ua = 'UA-XXXXX-X';
      
      var disqus_shortname = 'example';
      
      var enable_pjax = true;

      // Pace Options
      // ==============
      window.paceOptions = {
        catchupTime: 100,
        minTime: 100,
        elements: false,
        restartOnRequestAfter: 500,
        startOnPageLoad: false
      }

      // Ghostium Globals
      // ==============
      window.GHOSTIUM = {};
      GHOSTIUM.haveGA = typeof ga_ua !== 'undefined' && ga_ua !== 'UA-XXXXX-X';
      GHOSTIUM.haveDisqus = typeof disqus_shortname !== 'undefined' && disqus_shortname !== 'example';
      GHOSTIUM.enablePjax = typeof enable_pjax !== 'undefined' ? enable_pjax : true;
    </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/head-scripts.min.js?v=1483642327254"></script>

    <link rel="canonical" href="https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Ragha&#x27;s Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Exploring alternative neural computational models." />
    <meta property="og:description" content="Despite different variations of neural networks architectures, the computational model of a neuron hasn&amp;#8217;t changed since inception. Every neuron has \(n\) incomming inputs \(X &#x3D; (x_{1}, x_{2}, &amp;#8230;&amp;#8203;, x_{n})\) with weights \(W &#x3D; (w_{1}, w_{2}, &amp;#8230;&amp;#8203;, w_{n})\). A neuron then computes \(W." />
    <meta property="og:url" content="https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html" />
    <meta property="article:tag" content="deep learning" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Exploring alternative neural computational models." />
    <meta name="twitter:description" content="Despite different variations of neural networks architectures, the computational model of a neuron hasn&amp;#8217;t changed since inception. Every neuron has \(n\) incomming inputs \(X &#x3D; (x_{1}, x_{2}, &amp;#8230;&amp;#8203;, x_{n})\) with weights \(W &#x3D; (w_{1}, w_{2}, &amp;#8230;&amp;#8203;, w_{n})\). A neuron then computes \(W." />
    <meta name="twitter:url" content="https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html" />
    
    <script type="application/ld+json">
null
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="Ragha&#x27;s Blog" href="https://raghakot.github.io/rss/" />
  </head>
  <body class="post-template tag-deep-learning">

    <button data-action="open-drawer" id="drawer-button" class="drawer-button"><i class="fa fa-bars"></i></button>
    <nav tabindex="-1" class="drawer">
      <div class="drawer-container">
        <!--.drawer-search(role="search")-->
        <ul role="navigation" class="drawer-list">
          
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io" data-pjax>
              <i class="fa fa-home"></i>Home
            </a>
          </li>
          <!-- <li class="drawer-list-item">
            <a href="https://raghakot.github.io" title="Ragha&#x27;s Blog" data-pjax>
              <i class="fa fa-list-alt"></i>All posts
            </a>
          </li> -->
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io/rss/">
              <i class="fa fa-rss"></i>Subscribe to Feed
            </a>
          </li>
          <li class="drawer-list-divider"></li>
          <li class="drawer-list-item drawer-list-title">
            Follow me
          </li>
          
          
          <li class="drawer-list-item">
            <a href="https://github.com/raghakot" title="Github" target="_blank">
              <i class="fa fa-github"></i>Github
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://www.linkedin.com/in/raghavendra-kotikalapudi-79528411" title="LinkedIn" target="_blank">
              <i class="fa fa-linkedin"></i>LinkedIn
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="mailto:ragha@outlook.com" title="Email" target="_blank">
              <i class="fa fa-envelope-o"></i>Email
            </a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="drawer-overlay"></div>
    <main id="container" role="main" class="container">
      <div class="surface">
        <div class="surface-container">
          <div data-pjax-container class="content">
            
<section class="wrapper wrapper-post">
  <div class="wrapper-container">
    <article itemscope itemtype="http://schema.org/BlogPosting" role="article" class="post post tag-deep-learning">
        <section class="post-container">
          <header class="post-header">
            <ul class="post-meta-list">
              <li class="post-meta-item">
                <time datetime="2017-01-03" itemprop="datePublished">
                  3 days ago
                </time>
              </li>
                <li class="post-meta-item">
                  <span class="tags"><i class="fa fa-tags"></i>
                      <span>
                      <a href="https://raghakot.github.io/tag/deep-learning/">deep learning</a></span>
                  </span>
                </li>
              <li class="post-meta-item">
                <a href="#disqus_thread" data-disqus-identifier="">Comments</a>
              </li>
            </ul>
            <h1 itemprop="name headline" class="post-title"><a href="https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html" itemprop="url" data-pjax title="Exploring alternative neural computational models.">Exploring alternative neural computational models.</a></h1>
            <!--h2 itemprop="about" class="post-subtitle"></h2-->
          </header>
          <aside class="post-side">
            <div class="post-author">
                <a href="" class="post-author-avatar">
                  <img src="https://avatars.githubusercontent.com/u/15642444?v&#x3D;3" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <a href="" class="post-author-name">
                  Raghavendra Kotikalapudi
                </a>
                <p class="post-author-bio"></p>
              </div>
            </div>
          </aside>
          <div itemprop="articleBody" class="post-body">
            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Despite different variations of neural networks architectures, the computational model of a neuron hasn&#8217;t changed since inception. Every neuron has \(n\) incomming inputs \(X = (x_{1}, x_{2}, &#8230;&#8203;, x_{n})\) with weights \(W = (w_{1}, w_{2}, &#8230;&#8203;, w_{n})\). A neuron then computes \(W.X = \sum_{i=0}^{n}w_{i}.x_{i}\), forwards it through some nonlinearity like sigmoid or ReLU to generate the output<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup>.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/sim_neuron/neuron_model.jpeg" alt="neuron_model" width="400">
</div>
<div class="title">Figure 1. Computation model of a neuron</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>As far as i am aware, all architectures build on this fundamental computational model. Let us examine what the computation is conceptually doing in a step by step fashion.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Each input component \(x_{i}\) is being magnified or diminished when multiplied with a scaling factor \(w_{i}\). This can be interpreted as enhancing or diminishing the contribution of some pattern component \(x_{i}\) to the output decision. Think of these weights as <em>knobs</em> that range from \([-\infty, +\infty]\). The task of learning is to <em>tune</em> these knobs to the correct value.</p>
</li>
<li>
<p>The computation \(W.X\) is then enhancing/diminishing various input components of vector \(X\). In terms of linear algebra, the dot product of \(W\) and \(X\) can be interpeted as computing similarity. Consider \(X = (x_{1}, x_{2})\) and \(W = (w_{1}, w_{2})\). If you imagine plotting these vectors on a 2D graph paper, \(\frac{W.X}{|W||X|}\) is the cosine of angle between these two vectors. When these two vectors align (point in the same direction), the value of \(W.X\) is maximized indicating high degree of similarity. From this point of view, neuron is fundametally a pattern matching unit which outputs a large value when it matches a specific pattern (encoded by weights) in the input<sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup>.</p>
</li>
<li>
<p>In a complex neural network, every neuron is matching the input against some template encoded via weights and forwarding that result to the subsequent layers. In case of image recognition, the first layer neurons might match whether the input contains vertical edge, horizontal edge and so on. The circle detection neuron might use various edge matching computation results from previous layers to detect the shape patterns.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So far so good. <em>What is the purpose of activation function?</em> To understand the motivation, consider the geometric perspective of the computation through the lens of linear algebra.</p>
</div>
<div class="paragraph">
<p>\( W.X = \begin{bmatrix} w_{1} \\ w_{2} \\ \vdots \\ w_{n} \end{bmatrix} \begin{bmatrix} x_{1} &amp; x_{2} &amp; \cdots &amp; x_{n} \end{bmatrix} \)</p>
</div>
<div class="paragraph">
<p>Geometrically, this can be interpreted as a linear transformation of \(X\). Additionally, there is a bias term to shift the origin. So, the computation \(W.X + b\) can be interpreted as an affine transformation. Without an non-linear activation function, multiple linear transformations within each layer \(W_{L}\) can be condensed into a single layer with \(W = \prod_{L}W_{L}\). A single layered linear neural network is pretty limited in its representational power.</p>
</div>
<div class="paragraph">
<p>Lets put all this togther to interpret what a multi-layered neural network might be doing<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Suppose we input a \(100 \times 100\) image to a vggnet. The input tensor \(X\) can be thought of as a vector with 10000 basis. Each basis is representing some positional component in an image.</p>
</li>
<li>
<p>We know that the very first conv layer learns gabor filter like edge detection weight templates. Lets assume that the first convolutional layer filter generates \(100 \times 100\) output (assuming appropriate padding). This can be seen as <em>transforming</em> input from positional basis vector space to edge-like basis vector space. Insead of measuring the pixel intensity, the new vector space measures the similarity of the positional input patch with respect to a edge-detection templates (\(3 \times 3\) conv filter weights). Extrapolating on this notion, every layer <em>transforms</em> the data into a more meaningful basis vector space, eventually leading to basis vectors comprising of output categories.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_motivation">Motivation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the current computational model, the output of a neuron is different even though \(X\) and \(W\) perfectly align with each other but differ in magnitudes. As each \(w_{i}\) has range \([-\infty, \infty]\), backprop is essentially searching for this weight vector spanning all of the weight vector space.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/sim_neuron/vector_similarity.png" alt="vector_similarity" width="300">
</div>
<div class="title">Figure 2. Similarity between vectors X and W</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>If we view a neuron as fundamental template matching unit, then various magnitude (length) of \(X\) and \(W\) should yield the same output. If we constrain \(W\) to be a unit vector (\(|W| = 1\)), then backprop would only need to search along points on a hypersphere of unit radius. Although the hypersphere contains infinitely many points, with sufficient discretization, the search space intuitively <em>feels</em> a lot smaller than the entire weight vector space.</p>
</div>
<div class="paragraph">
<p>Considering that weight sharing (a form on constraint) in conv-nets led to huge improvements, constraining seems like a resonable approach. Weight regularization - a form of constraint - penalizes large \(|W|\) and vaguely fits the idea of encouraging weight updates to explore points along the hypersphere.</p>
</div>
<div class="paragraph">
<p>I think that in general, constraining as a research direction is largely unexplored. For example, it might be interesting to try and constrain conv filters to be as <em>distinct</em> and <em>diverse</em> and possible. I will follow up on these investigations in a future blog.</p>
</div>
<div class="paragraph">
<p>Getting back to the topic at hand, there are several options for constraining \(|W| = 1\).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Penalize when \(|W|\) deviates from 1 as a regularization loss.</p>
</li>
<li>
<p>Represent \(W\) in parametric form. Each \(w_{i}\) can be calculated using n-dim <a href="https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates">spherical coordinates</a>.</p>
</li>
<li>
<p>Instead of computing \(W.X\), compute \(\frac{W}{|W|} \times X\). Normalizing \(W\) ensures that the output does not change due to scaling. Consequently, the gradient of output with respect to \(W\) can only exist if \(W\) changes along the unit hypersphere.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_experiments">Experiments</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I explored option 3 as it was the simplest to implement. The architecture is a simplified version of vggnet comprising of \(3 \times 3\) convolutions with ReLU activation and max pooling. I used cifar10 dataset augmented with 10% random shifts along image rows/cols along with a 50% chance of horizontal flip. <code>random_seed = 1337</code> was used to get consistent and reproducable results across trials.</p>
</div>
<div class="paragraph">
<p>The model has 1,250,858 parameters and trained for 50 epochs with a batch size of 32 using categorical crossentropy loss with Adam optimizer.</p>
</div>
<div class="paragraph">
<p>\(W_{norm}\) is calculated as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python"># 1e-8 is used to prevent division by 0
W_norm = W / (tf.sqrt(tf.reduce_sum(tf.square(W), axis=[0, 1, 2], keepdims=True)) + 1e-8)</code></pre>
</div>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/sim_neuron/model.png" alt="test_model" width="300">
</div>
<div class="title">Figure 3. Test model</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_results">Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Final loss and accuracy values on validation set are summarized in the table.</p>
</div>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 1. Convergence results after 50 epochs</caption>
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top"></th>
<th class="tableblock halign-left valign-top">Old Model</th>
<th class="tableblock halign-left valign-top">New Model</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val_loss</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.8257</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.6156</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">val_accuracy</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.7165</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">0.7935</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>As hypothesized, constraining weight vector to a unit hypersphere speeds up training (see convergence graphs).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/sim_neuron/convergence.png" alt="convergence_graphs" width="800">
</div>
<div class="title">Figure 4. Convergence graphs for loss and accuracy on validation set for <span class="aqua">old</span> and <span class="red">new</span> computational models</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We saw pretty good improvements by making a relatively simple change to the neuron computation model. Experiments used <code>ReLU</code> which effectively attenuates negative values. This limits the neuron to only communicate information when the angle between \(X\) and \(W\) lies between \([-\frac{\pi}{2}, \frac{\pi}{2}]\). Perhaps it is useful if a neuron could also communicate the <em>lack of</em> similarity or the <em>inverse</em> of weight template information. For example, the lack of a specific stripe pattern might increase the networks confidence that the output is more likely to be one cat species over another.</p>
</div>
<div class="paragraph">
<p>One way to remedy this problem might be to use an activation function that allows negative values. A quick experiment with <code>ELU</code> activation, however, did not show any significant improvement over <code>ReLU</code>.</p>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. Technically, bias is involved, but i am excluding it to keep the discussion focused.
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. The correct weight vectors are learned using backpropogation.
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. This is my own interpretation and might as well be incorrect.
</div>
</div>
          </div>
          <footer class="post-footer">
            <div itemprop="author" itemscope itemtype="http://schema.org/Person" class="post-author">
                <a href="" class="post-author-avatar">
                  <img itemprop="image" src="https://avatars.githubusercontent.com/u/15642444?v&#x3D;3" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <h4 class="post-footer-heading">Written By</h4>
                <a href="" itemprop="url" class="post-author-name">
                  <span itemprop="name">Raghavendra Kotikalapudi</span>
                </a>
                <p itemprop="description" class="post-author-bio"></p>
                  <p class="post-author-location">Seattle WA</p>
                <p class="post-info">
                  <b class="post-info-title">Published on</b>
                  <time class="post-date">January 03, 2017</time>
                </p>
              </div>
            </div>
            <div class="post-social">
              <h4 class="post-footer-heading">Spread the word</h4>
              <a href="#" data-action="share-twitter"><i class="fa fa-fw fa-lg fa-twitter"></i></a>
              <a href="#" data-action="share-facebook"><i class="fa fa-fw fa-lg fa-facebook"></i></a>
              <a href="#" data-action="share-gplus"><i class="fa fa-fw fa-lg fa-google-plus"></i></a>
            </div>
          </footer>
        </section>
      <section itemprop="comment" class="post-comments">
        <div id="disqus_thread"></div>
      </section>
    </article>

    <footer role="contentinfo" class="footer">
      <p><small>Copyright &copy; <span itemprop="copyrightHolder">Ragha&#x27;s Blog</span>. 2017. All Rights Reserved.</small></p>
      <p><small><a href="http://ghostium.oswaldoacauan.com/" target="_blank">Ghostium Theme</a> by <a href="http://twitter.com/oswaldoacauan" target="_blank">@oswaldoacauan</a></small></p>
      <p><small>Adapted by <a href="https://twitter.com/mgreau">Maxime Gréau</a></small></p>
      <p><small>Published with <a href="http://hubpress.io">HubPress</a></small></p>
    </footer>
  </div>
</section>


<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  var disqus_shortname = 'raghakot-github-io'; // required: replace example with your forum shortname
  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


          </div>
        </div>
      </div>
    </main>

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/foot-scripts.min.js?v=1483642327254"></script>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-78195880-1', 'auto');
    ga('send', 'pageview');

    </script>

  </body>
</html>
