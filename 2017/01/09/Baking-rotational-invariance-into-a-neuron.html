<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Baking rotational invariance into a neuron</title>
    <meta name="description" content="" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://raghakot.github.io/favicon.ico">

    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:400,700,400italic,700italic|Open+Sans:400italic,700italic,700,400">
    <link rel="stylesheet" type="text/css" href="//raghakot.github.io/themes/roon/assets/css/screen.css?v=1484257967325" />

    <link rel="canonical" href="https://raghakot.github.io/2017/01/09/Baking-rotational-invariance-into-a-neuron.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Ragha&#x27;s Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Baking rotational invariance into a neuron" />
    <meta property="og:description" content="This is a continuation from the first post on exploring alternative neural computational models. To recap quickly, a neuron is fundamentally acting as a pattern matching unit by computing the dot product \(W \cdot X\), which is equivalent to unnormalized cosine similarity between vectors \(W\) and \(X\). This model of" />
    <meta property="og:url" content="https://raghakot.github.io/2017/01/09/Baking-rotational-invariance-into-a-neuron.html" />
    <meta property="article:tag" content="deep learning" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Baking rotational invariance into a neuron" />
    <meta name="twitter:description" content="This is a continuation from the first post on exploring alternative neural computational models. To recap quickly, a neuron is fundamentally acting as a pattern matching unit by computing the dot product \(W \cdot X\), which is equivalent to unnormalized cosine similarity between vectors \(W\) and \(X\). This model of" />
    <meta name="twitter:url" content="https://raghakot.github.io/2017/01/09/Baking-rotational-invariance-into-a-neuron.html" />
    
    <script type="application/ld+json">
null
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="Ragha&#x27;s Blog" href="https://raghakot.github.io/rss/" />
</head>
<body class="post-template tag-deep-learning  noimage">

    


    <article role="main" class="">
        <header>
            <a href="https://raghakot.github.io" id="home_link">Â«</a>
            <div class="meta"><time datetime="2017-01-09"><a href="/">January 09, 2017</a></time> <span class="count" id="js-reading-time"></span></div>
            <h1>Baking rotational invariance into a neuron</h1>
        </header>

        <div class="text" id="js-post-content">
            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This is a continuation from the first <a href="https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html">post</a> on exploring alternative neural computational models. To recap quickly, a neuron is fundamentally acting as a pattern matching unit by computing the dot product \(W \cdot X\), which is equivalent to unnormalized cosine similarity between vectors \(W\) and \(X\).</p>
</div>
<div class="paragraph">
<p>This model of pattern matching is very naive, in a sense that it is not invariant to translations or rotations. Consider the following weight vector template (conv filter) corresponding to vertical edge pattern <code>|</code></p>
</div>
<div class="paragraph">
<p>\(W = \begin{bmatrix}
0.2 &amp; 1.0 &amp; 0.2\\
1.0 &amp; 1.0 &amp; 1.0\\
0.2 &amp; 1.0 &amp; 0.2
\end{bmatrix} \)</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/vertical_line_filter.png" alt="vertical_line_filter" width="300">
</div>
<div class="title">Figure 1. Visual representation of \(W\) template</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>When a neuron tries to match this pattern in a \(3 \times 3\) input image patch, it will only output a high value if a vertical edge is found in the center of the image patch. i.e., dot product is not invariant to shifts. My first thought was to use <a href="https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson coefficient</a> as it provides shift invariance. However, this not a problem in practice as the filter is slid across all possible \(3 \times 3\) patches within the input image (see fig2).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/conv_illustration.png" alt="conv_illustration" width="400">
</div>
<div class="title">Figure 2. Sliding filter over input image to generate the output activation area (image borrowed from <a href="http://intellabs.github.io/RiverTrail/tutorial/">river trail documentation</a>)</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>In a way, the sliding window operation is quite clever as it communicates information about <em>parts of the image</em> matching filter template without increasing the number of <em>trainable</em> parameters.  As a concrete example, consider a conv-net with filters as shown in fig3<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup>. The first layer filters (green) detect eyes, nose etc., second layer filters (yellow) detect face, leg etc., by aggregating features from first layer and so on<sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup>.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/conv_net1.png" alt="conv_illustration" width="600">
</div>
<div class="title">Figure 3. Hypothetical conv-net filters to illustrate how a <em>human</em> might be detected. (image borrowed from quora <a href="https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features">post</a>)</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>Now suppose the face (represented by two red and one magenta point) is moved to another corner as shown in fig4. The same activations occur, leading to the same end result.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/conv_net2.png" alt="conv_illustration" width="600">
</div>
<div class="title">Figure 4. Illustration of activations when an input object is shifted. (image borrowed from quora <a href="https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features">post</a>)</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>So far so good. How about rotational invariance? We know that a lot of learned conv filters are identical, but rotated by some non-random factor. Instead of learning rotational invariance in this manner, we will try to build it directly into the computational model. The motivation is simple - the neuron should output high value even if some rotated version of pattern is present in the input. Hopefully this eliminates redundant filters and improves test time accuracy.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/conv_filters.jpg" alt="conv_filters" width="600">
</div>
<div class="title">Figure 5. A few conv filters of a vggnet (image borrowed from keras <a href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html">blog post</a>). Notice how some filters are identical but rotated.</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_endowing_rotational_invariance">Endowing rotational invariance</h2>
<div class="sectionbody">
<div class="paragraph">
<p>How can we build rotational invariance into dot product similarity computation? After searching for a bit on the Internet, I couldn&#8217;t find any obvious similarity metrics that do this. Sure there is <a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform">SIFT</a> and <a href="https://en.wikipedia.org/wiki/Speeded_up_robust_features">SURF</a>, but they are complex computations and cannot be expressed in terms of dot products alone.</p>
</div>
<div class="paragraph">
<p>Instead of trying to come up with a metric, I decided to try the brute force approach of matching the input patch with all possible rotations of the filter. If we take <code>max</code> of all those outputs, then, in principle, we are choosing the output resulting from the rotated filter that best <code>aligns</code> with the pattern in input patch. This strategy is partly inspired by how conv nets gets around translational invariance problem by simply sliding the filter over all possible locations. Unlike sliding operation, which is an <em>architectural</em> property rather than the computational property of the neuron, the brute force rotations of filters can be confined within the abstraction of neuron, making it a part of the computational model.</p>
</div>
<div class="paragraph">
<p>Lets look at this idea in more concrete terms. A neuron receives input \(X\) and has associated weights \(W\). Let \(r\) denote some arbitrary discrete rotation \(\theta\). Instead of directly computing \(W \cdot X\), we will compute \(\max \{ rW \cdot X, r^{2}W \cdot X, \cdots, r^{k}W \cdot X \}\), where \(k = \left \lceil \frac{360}{\theta} \right \rceil\) is the number of brute force rotations of \(\theta\) step.</p>
</div>
<div class="paragraph">
<p>This idea has a number of practical challenges.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>How do we decide \(\theta\)? If we set \(\theta = 90^{\circ}\), we can produce 4 rotated filters.</p>
</li>
<li>
<p>For \(\theta \neq 90^{\circ}\) we need interpolation, which might change filter shape.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To avoid interpolation, we can try to shift the image by rotating elements clockwise from outermost layer to innermost layer. For a \(3 \times 3\) image, there are 8 possible rotations. Unfortunately, this only works for filters that are symmetric along the center of the matrix. As an example, consider all 8 rolled rotations of the <code>L</code> shaped filter (fig 6). As the image is not symmetric along the center, we get skewed representations for \(45\circ\) rotations. The second image in fig 6 still looks like an <code>L</code>, except that the tail end of <code>L</code> is squished upwards to maintain \(3 \times 3\) filter shape.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/rotations_L.png" alt="rotations_of_filter" width="600">
</div>
<div class="title">Figure 6. All 8 shifts of the <code>L</code> shaped filter. Each row contains a clockwise rolled elements from the previous.</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>Despite the setback, all \(90^{\circ}\) rotations are accurate and \(45^{\circ}\) rotations are only slightly skewed. Skew might be a blessing in disguise as it might endow the neuron to be <em>deform</em> invariant as well (wishful thinking). In either case, the deformations are not too bad for \(3 \times 3\) filters, and it is at-least worth experimenting with vggnet which uses all \(3 \times 3\) conv filters.</p>
</div>
<div class="paragraph">
<p>Alternatively, instead of trying all possible alignments, we could try to find \(\theta\) that <em>maximizes</em> the dot product.</p>
</div>
<div class="paragraph">
<p>Let R denote \(2 \times 2\) rotation matrix.</p>
</div>
<div class="paragraph">
<p>\(R = \begin{bmatrix}
cos(\theta) &amp; sin(\theta)\\
-sin(\theta) &amp; cos(\theta)
\end{bmatrix}\)</p>
</div>
<div class="paragraph">
<p>To accomplish optimal alignment, we need \(\theta\) such that:</p>
</div>
<div class="paragraph">
<p>\(\arg\max_\theta R_{\theta}W \cdot X\)</p>
</div>
<div class="paragraph">
<p>This can be solved by differentiating with respect to \(\theta\) and equating the derivative to 0.</p>
</div>
<div class="paragraph">
<p>I finally decided to go with the brute force approach to quickly test out the idea and maybe further develop the math for optimal alignment if it showed promise. The brute-force computations discussed so far apply to an individual neuron. For Convolutional layer, the output is generated as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Generate all 8 rotated filters from \(W\), \(W_{1}, \cdots, W_{8}\).</p>
</li>
<li>
<p>Compute output activation volumes<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup> \(O_{1}, \cdots, O_{8}\) by convolving input with \(W_{1}, \cdots, W_{8}\).</p>
</li>
<li>
<p>Across volumes, select \(x, y\) value corresponding to filter \((row, col)\) that has the max value. This corresponding to selecting the best <em>aligned</em> filter with input patch across the 8 rotations.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These steps can be implemented as follows. Note that <code>W.shape = (rows, cols, nb_input_filters, nb_output_filters)</code>. You also need bleeding edge tensorflow from <em>master</em>; as of Nov 2016, <code>gather_nd</code> does not have a gradient implementation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import tensorflow as tf


# The clockwise shift-1 rotation permutation.
permutation = [[1, 0], [0, 0], [0, 1], [2, 0], [1, 1], [0, 2], [2, 1], [2, 2], [1, 2]]


def shift_rotate(w, shift=1):
    shape = w.get_shape()
    for i in range(shift):
        w = tf.reshape(tf.gather_nd(w, permutation), shape)
    return w


def conv2d(x, W, **kwargs):
    # Determine all 7 rotations of w.
    w = W
    w_rot = [w]
    for i in range(7):
        w = shift_rotate(w)
        w_rot.append(w)

    # Convolve with all 8 rotations and stack.
    outputs = tf.stack([tf.nn.conv2d(x, w_i, **kwargs) for w_i in w_rot])

    # Max filter activation across rotations.
    output = tf.reduce_max(outputs, 0)
    return output</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_experimental_setup">Experimental Setup</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Mini vggnet comprising of \(3 \times 3\) convolutions with ReLU activation.</p>
</li>
<li>
<p>cifar10 dataset augmented with 10% random shifts along image rows/cols along with a 50% chance of horizontal flip.</p>
</li>
<li>
<p><code>random_seed = 1337</code> for reproducibility.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The model has 1,250,858 parameters and trained for 50 epochs with a batch size of 32 using categorical cross-entropy loss with Adam optimizer.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural1/model.png" alt="test_model" width="300">
</div>
<div class="title">Figure 7. Test model</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_discussion">Discussion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Disappointingly, rotational invariant model resulted in lower val accuracy than the baseline. I had three running hypothesis as to why this might happen.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Implementation issue. I ruled this out by testing the <code>conv2d(&#8230;&#8203;)</code> implementation by feeding it numpy arrays and verifying the output.</p>
</li>
<li>
<p>Perhaps, rotational invariance is not a good idea for the earlier filters. As an example, consider a latter layer that uses <em>vertical</em> and <em>horizontal</em> edge features for the preceeding layer to detect a <em>square</em> like shape. Having rotational invariance means that it is not possible to distinguish horizontal from vertical edges, making square detection a lot more challenging.</p>
</li>
<li>
<p>Maybe the skew from \(\theta = \{45^{\circ}, 135^{\circ}, 225^{\circ}, 315^{\circ}\}\) was an issue after all.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>To test 2, 3, I decided to run 6 more experiments by using:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>\(90^{\circ}\) rotations (4 filters) and \(45^{\circ}\) rotations (8 filters)</p>
</li>
<li>
<p>Use new conv logic only on all four conv layers (all), last two conv layers (half), and the last conv layer (third).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The results of these experiments are summarized in fig 8.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/convergence.png" alt="convergence" width="800">
</div>
<div class="title">Figure 8. Convergence graphs for #{8/4 rotations}_rot_{all/half/third} and the baseline model (old).</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>It seems evident that using rotationally invariant convolutional layers everywhere is not a good idea (<em>4_rot_all</em> and <em>8_rot_all</em> have the lowest values). In accordance with hypothesis 2, the accuracy improved as we get rid of earlier rotationally invariant conv layers, with 4_rot_third and 8_rot_third almost catching up to baseline model accuracy. The skew (according to hypothesis 3) does not seem to the issue as <em>8_rot_#</em> closely followed <em>4_rot_#</em> accuracies.</p>
</div>
<div class="paragraph">
<p>Fortunately, there is a silver lining despite all the setbacks. fig 9 shows the plot of prediction probabilities for all three models across rotated test cifar10 image by step of \(1^{\circ}\) from \(\theta = \{0, 360\}\). Adding rotationally invariant conv net to last layer (maybe last few for a larger conv net?) seems to make model robust to input image rotations, with the same convergence rate as the baseline model. It also appears that \(90^{\circ}\)  rotations are yielding most bang per computation.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/alt_neural2/rotation_plot.png" alt="rotation_plot" width="800">
</div>
<div class="title">Figure 9. Plot of prediction probability of the correct output class as the input image is rotated from \(\theta = \{0, 360\}\) by step of \(1^{\circ}\) across models. Legend shows the percentage of times the class was correctly predicted.</div>
</div>
<div class="paragraph">
<p><br></p>
</div>
<div class="paragraph">
<p>Curiously, even the baseline model shows better robustness to rotations if i take a pretrained baseline model but add rotational conv layer (this can be done because weights are shared across rotations). Table compares the % of accurate classification across \(\theta = \{0, 360\}\) with \(1^{\circ}\) step.</p>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. First layer filters generally detect basic lines and textures, this is just a simplified view to illustrate the point
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. In reality, convolution filters may detect objects that have no meaningful interpretation
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. If you are not familiar with the idea of activation volumes, I would recommend referring to <a href="http://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual Recognition</a> for an outstanding explanation.
</div>
</div>
        </div>

        <menu>
            <a href="" id="btn_share" class="btn" title="Share">
                <span aria-hidden="true" data-icon="S"></span>
                <strong>Share</strong>
            </a>
            <a href="http://twitter.com/share?text=Baking%20rotational%20invariance%20into%20a%20neuron&url=https://raghakot.github.io/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" id="btn_comment" class="btn" target="_blank">
                <span aria-hidden="true" data-icon="C"></span> Comment on Twitter
            </a>
        </menu>


        <footer class="post-footer" role="contentinfo">

            <div class="vcard">
                <a href="https://raghakot.github.io/rss" id="btn_feed" class="btn" title="Feed" target="_blank">
                    <span aria-hidden="true" data-icon=")"></span>
                    <strong>Feed</strong>
                </a>

                <a href="https://raghakot.github.io/author/raghakot/" class="photo">
                    <span style="background-image: url('https://avatars.githubusercontent.com/u/15642444?v&#x3D;3');">
                        <img src="https://avatars.githubusercontent.com/u/15642444?v&#x3D;3" alt="Raghavendra Kotikalapudi">
                    </span>
                </a>

                <div class="details">
                    <h4><a href="https://raghakot.github.io/author/raghakot/" class="url n">Raghavendra Kotikalapudi</a></h4>
                    Seattle WA<br>
                    
                </div>
            </div>

            <div id="user_bio">
                <div class="inner">
                    
                </div>
            </div>

        </footer>




    <section class="post-comments">
      <div id="disqus_thread"></div>
      <script type="text/javascript">
      var disqus_shortname = 'raghakot-github-io'; // required: replace example with your forum shortname
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </section>


    </article>

    <div id="share_modal">
        <div class="wrap">
            <div class="inner">
                <header>
                    Share
                    <a href="" class="close" title="Close">&times;</a>
                </header>

                <div class="roon-share-links">
                    <a href="https://twitter.com/share" class="twitter-share-button" data-dnt="true">Tweet</a>
                    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

                    <div id="fb-elems">
                        <div id="fb-root"></div>
                        <script>(function(d, s, id) {
                        var js, fjs = d.getElementsByTagName(s)[0];
                        if (d.getElementById(id)) return;
                        js = d.createElement(s); js.id = id;
                        js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=463438580397968";
                        fjs.parentNode.insertBefore(js, fjs);
                        }(document, 'script', 'facebook-jssdk'));</script>
                        <div class="fb-like" data-send="false" data-layout="button_count" data-width="110" data-show-faces="false" data-font="arial"></div>
                    </div>

                    <div id="pinit-btn">
                        <a href="//pinterest.com/pin/create/button/?url=https://raghakot.github.io/&amp;description=Baking%20rotational%20invariance%20into%20a%20neuron-Ragha's%20Blog " data-pin-do="buttonPin" data-pin-config="beside"><img src="//assets.pinterest.com/images/pidgets/pin_it_button.png"></a>
                        <script type="text/javascript" src="//assets.pinterest.com/js/pinit.js"></script>
                    </div>
                </div>
            </div>
        </div>
    </div>






    <script>

            function get_text(el) {
                ret = "";
                var length = el.childNodes.length;
                for(var i = 0; i < length; i++) {
                    var node = el.childNodes[i];
                    if(node.nodeType != 8) {
                        ret += node.nodeType != 1 ? node.nodeValue : get_text(node);
                    }
                }
                return ret;
            }
            function reading_time () {
                var post_content = document.getElementById('js-post-content');
                if (post_content) {
                    var words = get_text(post_content),
                        count = words.split(/\s+/).length,
                        read_time = Math.ceil((count / 150)),
                        read_time_node = document.createTextNode(read_time + ' min read');
                    document.getElementById('js-reading-time').appendChild(read_time_node);
                }
            }

        function no_schema_links () {
            var links = document.querySelectorAll('.js-remove-domain-schema');
            if (links) {
                for (i = 0; i < links.length; ++i) {
                    var link = links[i],
                        text = link.innerHTML,
                        no_schema = text.replace(/.*?:\/\//g, "");
                    link.innerHTML = no_schema;
                }
            }
        }

        window.onload = function () {
            no_schema_links();

            reading_time();
        }
    </script>

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>

        <script>
            $(function(){
                var share_modal = $("#share_modal"),
                    update_social_links = true;

                $("#btn_share").click(function(){
                    var that = $(this);
                    share_modal.fadeIn(200);
                    return false;
                });

                share_modal.click(function(e){
                    if (e.target.className == "wrap" || e.target.id == "share_modal") {
                        share_modal.fadeOut(200);
                    }
                    return false;
                });

                share_modal.find("div.inner > header > a.close").click(function(){
                    share_modal.fadeOut(200);
                    return false;
                });
            });
        </script>


    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-78195880-1', 'auto');
    ga('send', 'pageview');

    </script>

</body>
</html>
