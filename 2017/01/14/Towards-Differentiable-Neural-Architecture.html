<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <title>Towards Differentiable Neural Architecture</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Towards Differentiable Neural Architecture">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Towards Differentiable Neural Architecture">
    <meta property="og:description" content="">

    <!-- <meta name="twitter:site" content="">

<meta name="twitter:creator" content="">

<meta name="google-site-verification" content="">

<meta property="fb:admins" content="">
 -->

    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="/apple-touch-icon-precomposed.png" rel="apple-touch-icon">

    <link href="//fonts.googleapis.com/" rel="dns-prefetch">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Open+Sans:700,400&subset=latin,latin-ext" rel="stylesheet">

    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/main.min.css?v=1485157443797"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/custom.css?v=1485157443797"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/asciidoctor-foundation.css?v=1485157443797"/>




    <script type="text/javascript">
      var ga_ua = 'UA-XXXXX-X';
      
      var disqus_shortname = 'example';
      
      var enable_pjax = true;

      // Pace Options
      // ==============
      window.paceOptions = {
        catchupTime: 100,
        minTime: 100,
        elements: false,
        restartOnRequestAfter: 500,
        startOnPageLoad: false
      }

      // Ghostium Globals
      // ==============
      window.GHOSTIUM = {};
      GHOSTIUM.haveGA = typeof ga_ua !== 'undefined' && ga_ua !== 'UA-XXXXX-X';
      GHOSTIUM.haveDisqus = typeof disqus_shortname !== 'undefined' && disqus_shortname !== 'example';
      GHOSTIUM.enablePjax = typeof enable_pjax !== 'undefined' ? enable_pjax : true;
    </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/head-scripts.min.js?v=1485157443797"></script>

    <link rel="canonical" href="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Ragha&#x27;s Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Towards Differentiable Neural Architecture" />
    <meta property="og:description" content="Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture e" />
    <meta property="og:url" content="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    <meta property="article:tag" content="deep learning" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Towards Differentiable Neural Architecture" />
    <meta name="twitter:description" content="Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture e" />
    <meta name="twitter:url" content="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    
    <script type="application/ld+json">
null
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="Ragha&#x27;s Blog" href="https://raghakot.github.io/rss/" />
  </head>
  <body class="post-template tag-deep-learning">

    <button data-action="open-drawer" id="drawer-button" class="drawer-button"><i class="fa fa-bars"></i></button>
    <nav tabindex="-1" class="drawer">
      <div class="drawer-container">
        <!--.drawer-search(role="search")-->
        <ul role="navigation" class="drawer-list">
          
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io" data-pjax>
              <i class="fa fa-home"></i>Home
            </a>
          </li>
          <!-- <li class="drawer-list-item">
            <a href="https://raghakot.github.io" title="Ragha&#x27;s Blog" data-pjax>
              <i class="fa fa-list-alt"></i>All posts
            </a>
          </li> -->
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io/rss/">
              <i class="fa fa-rss"></i>Subscribe to Feed
            </a>
          </li>
          <li class="drawer-list-divider"></li>
          <li class="drawer-list-item drawer-list-title">
            Follow me
          </li>
          
          
          <li class="drawer-list-item">
            <a href="https://github.com/raghakot" title="Github" target="_blank">
              <i class="fa fa-github"></i>Github
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://www.facebook.com/raghavendra.kotikalapudi" title="Facebook" target="_blank">
              <i class="fa fa-facebook"></i>Facebook
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://www.linkedin.com/in/raghavendra-kotikalapudi-79528411" title="LinkedIn" target="_blank">
              <i class="fa fa-linkedin"></i>LinkedIn
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="mailto:ragha@outlook.com" title="Email" target="_blank">
              <i class="fa fa-envelope-o"></i>Email
            </a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="drawer-overlay"></div>
    <main id="container" role="main" class="container">
      <div class="surface">
        <div class="surface-container">
          <div data-pjax-container class="content">
            
<section class="wrapper wrapper-post">
  <div class="wrapper-container">
    <article itemscope itemtype="http://schema.org/BlogPosting" role="article" class="post post tag-deep-learning">
        <section class="post-container">
          <header class="post-header">
            <ul class="post-meta-list">
              <li class="post-meta-item">
                <time datetime="2017-01-14" itemprop="datePublished">
                  9 days ago
                </time>
              </li>
                <li class="post-meta-item">
                  <span class="tags"><i class="fa fa-tags"></i>
                      <span>
                      <a href="https://raghakot.github.io/tag/deep-learning/">deep learning</a></span>
                  </span>
                </li>
              <li class="post-meta-item">
                <a href="#disqus_thread" data-disqus-identifier="">Comments</a>
              </li>
            </ul>
            <h1 itemprop="name headline" class="post-title"><a href="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" itemprop="url" data-pjax title="Towards Differentiable Neural Architecture">Towards Differentiable Neural Architecture</a></h1>
            <!--h2 itemprop="about" class="post-subtitle"></h2-->
          </header>
          <aside class="post-side">
            <div class="post-author">
                <a href="http://raghakot.github.io/" class="post-author-avatar">
                  <img src="https://avatars.githubusercontent.com/u/15642444?v&#x3D;3" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <a href="http://raghakot.github.io/" class="post-author-name">
                  Raghavendra Kotikalapudi
                </a>
                <p class="post-author-bio">Learning to machine learn</p>
              </div>
            </div>
          </aside>
          <div itemprop="articleBody" class="post-body">
            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture engineering is the new feature engineering and it clearly plays an important role in advancing the state of the art. Instead of making incremental improvements, is there a way to <em>learn</em> the connectivity pattern itself?</p>
</div>
<div class="paragraph">
<p>First step is to make this more concrete. Ideally, we want to learn connectivity amongst individual neurons; instead, lets simplify the problem by constraining ourselves to known layer Lego blocks (by layer Lego blocks, I mean general purpose computational layers such as <em>Convolutional Layer</em>, <em>LSTM</em>, <em>Pooling</em> etc.). Given a bunch of Layer Legos \(L = \{L_{1}, L_{2}, \cdots, L_{n}\}\), our task is to learn how these should be assembled. First thing that comes to mind is evolutionary search, but that is computationally prohibitive. A quicker and more elegant alternative would be to learn this by computing the gradient of <em>architecture</em> with respect to loss. The challenge is to make <em>architecture</em> itself differentiable.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/legos.jpg" alt="legos" width="400">
</div>
<div class="title">Figure 1. Deep learning practitioners</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_differentiable_architecture">Differentiable Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let us start by considering all possible ways in which \(n\) layers can be connected. This is equivalent to the maximum number of connections (edges) in a directed graph with \(n\) vertices, which is \(n(n-1)\). The simplest way to search amongst all these possibilities is to start with a fully connected network where each layer is connected to every other layer via some <em>weight</em> which can then be <em>learned</em> via backprop. Ideally, \(w \in [0, 1]\) to indicate whether a connection is being used or not.</p>
</div>
<div class="paragraph">
<p>Backward connections introduce cycles in the graph which are problematic<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup>. Excluding backward connections, every layer can have incoming inputs from all previous layers, which leaves us with \(\frac{n(n-1)}{2}\) possible connections. This is exactly what <a href="https://arxiv.org/pdf/1608.06993v3.pdf">DenseNets</a> do!, except that we arrived at a similar architecture with a different motivation.</p>
</div>
<div class="paragraph">
<p>Unlike DenseNets, we want the connections to be gated so that they can be pruned if needed. To mostly allow or disallow inputs, we can use \(sigmoid(W)\) as they tend to output values closer to 0 or 1. This is in similar spirit to how LSTMs use the <em>forget gate layer</em> <sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup>, but without a neural network. For every outgoing edge from layer \(L\), we will associate a weight \(W = [w_{1}, w_{2}, \cdots, w_{f}]\), where \(f\) is the number of feature maps for layer \(L\). For a layer with incoming edges, we can simply compute the weighted feature maps from each incoming connection.</p>
</div>
<div class="paragraph">
<p>Suppose Layer \(L_{i}\) gets two feature maps \(f_{1}, f_{2}\) weighted with \(w_{1}, w_{2}\). There are a couple of options to define the computation of \(L_{i}\) in terms of input feature maps.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>DenseNet style concatenation. i.e., \(Output = F([f_{1}, f_{2}])\). This has the limitation that concatenation is not viable when the size of feature maps differ. One option is to down sample all input feature maps to the smallest feature map (since feature maps from prev layers are usually larger in width and height dimension). Once all feature maps are of the same shape, we could try to condense them into a single feature map using <em>avg</em>, <em>max</em> or <em>min</em> operation across filters. Intuitively, <em>max</em> makes sense since it is equivalent to focusing on input patterns that matched with a filter (high output value) across various feature maps. A high value at some \((row, col)\) of the filter might indicate the presence of some abstract shape detected by previous convolutional layers.</p>
</li>
<li>
<p>ResNet style summation. i.e., \(Output = F(f_{1} + f_{2})\). I somehow dislike this idea as i cannot reconcile what it means to sum activations over arbitrary feature maps. It is still worth experimenting. We can also try various forms of merging using \([+, -, \times]\).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>That completes our setup. To summarize, the plan is to start out with an fully connected network where every layer gets inputs from all the previous layers in a weighted fashion. Before weighing, we use \(sigmoid(W)\) to bias the values mostly towards 0 or 1. When trained end-end via backprop, the network should, in-principle, learn to prune unwanted connections.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_experimental_setup">Experimental Setup</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Mini vggnet comprising of \(3 \times 3\) convolutions with ReLU activation.</p>
</li>
<li>
<p>cifar10 dataset augmented with 10% random shifts along image rows/cols along with a 50% chance of horizontal flip.</p>
</li>
<li>
<p><code>random_seed = 1337</code> for reproducibility.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The model has 1,250,858 parameters and trained for 50 epochs with a batch size of 32 using categorical cross-entropy loss with Adam optimizer.</p>
</div>
<div class="paragraph">
<p><strong>Experiments</strong></p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Connection weights with/without sigmoid.</p>
</li>
<li>
<p>DenseNet style input aggregation</p>
<div class="ulist">
<ul>
<li>
<p>Concat feature maps after reshaping input filters to same shape</p>
</li>
<li>
<p>Concat followed by max/min/avg across all inputs.</p>
</li>
</ul>
</div>
</li>
<li>
<p>ResNet style merge with \([+, -, \times]\).</p>
</li>
<li>
<p>Connection weight initialization schemes.</p>
<div class="ulist">
<ul>
<li>
<p>Init all weights to 1.</p>
</li>
<li>
<p>Init all weights using gaussian centered at 0.5</p>
</li>
<li>
<p>Create an initial feed forward stack by init weights between \(L_{i-1}\) and \(L_{i}\) to 1 and set everything else to 0 or Gaussian centered at 0.5.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_implementation">Implementation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A quick summary of these ideas are translated into concrete implementation. A complete implementation can be found on my <a href="https://github.com/raghakot/deep-learning-experiments/tree/master/exp3">github</a>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Creating fully connected net</strong>. i.e, a layer is connected to all prev layer outputs. This is not as hard as it appears.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def make_fully_connected(x, layers):
    inputs = [x]
    for layer in layers:
        x = layer(x)
        inputs.append(x)
        # This is the part where we resize inputs to be of same shape and merge them in resnet or densenet style
    return x</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Merging</strong>. i.e., resizing prev layer outputs to be of the same shape and concatenating them in densenet or resnet style. We also want to weigh merged outputs so that those weights can be learned during backprop. The easiest way to do this in keras is to create a custom layer<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import numpy as np
import tensorflow as tf

from keras import backend as K
from keras.layers import Lambda, Layer

class Connection(Layer):
    """Takes a list of inputs, resizes them to the same shape, and outputs a weighted merge.
    """
    def __init__(self, init_value=0.5, merge_mode='concat',
                 resize_interpolation=tf.image.ResizeMethod.BILINEAR,
                 shared_weights=True):
        """Creates a connection that encapsulates weighted connection of input feature maps.

        :param init_value: The init value for connection weight
        :param merge_mode: Defines how feature maps from inputs are aggregated.
        :param resize_interpolation: The downscaling interpolation to use. Instance of `tf.image.ResizeMethod`.
            Note that ResizeMethod.AREA will fail as its gradient is not yet implemented.
        :param shared_weights: True to share the same weight for all feature maps from inputs[i].
        False creates a separate weight per feature map.
        """
        self.init_value = init_value
        self.merge_mode = merge_mode
        self.resize_interpolation = resize_interpolation
        self.shared_weights = shared_weights
        super(Connection, self).__init__()

    def _ensure_same_size(self, inputs):
        """Ensures that all inputs match last input size.
        """
        # Find min (row, col) value and resize all inputs to that value.
        rows = min([K.int_shape(x)[1] for x in inputs])
        cols = min([K.int_shape(x)[2] for x in inputs])
        return [tf.image.resize_images(x, [rows, cols], self.resize_interpolation) for x in inputs]

    def _merge(self, inputs):
        """Define other merge ops like [+, X, Avg] here.
        """
        if self.merge_mode == 'concat':
            # inputs are already stacked.
            return inputs
        else:
            raise RuntimeError('mode {} is invalid'.format(self.merge_mode))

    def build(self, input_shape):
        """ Create trainable weights for this connection
        """
        # Number of trainable weights = sum of all filters in `input_shape`
        nb_filters = sum([s[3] for s in input_shape])

        # Create shared weights for all filters within an input layer.
        if self.shared_weights:
            weights = []
            for shape in input_shape:
                # Create shared weight, make nb_filter number of clones.
                shared_w = K.variable(self.init_value)
                for _ in range(shape[3]):
                    weights.append(shared_w)
            self.W = K.concatenate(weights, axis=-1)
        else:
            self.W = K.variable(np.ones(shape=nb_filters) * self.init_value)

        self._trainable_weights.append(self.W)
        super(Connection, self).build(input_shape)

    def call(self, layer_inputs, mask=None):
        # Resize all inputs to same size.
        resized_inputs = self._ensure_same_size(layer_inputs)

        # Compute sigmoid weighted inputs
        stacked = K.concatenate(resized_inputs, axis=-1)
        weighted = stacked * K.sigmoid(self.W)

        # Merge according to provided merge strategy.
        merged = self._merge(weighted)

        # Cache this for use in `get_output_shape_for`
        self._out_shape = K.int_shape(merged)
        return merged

    def get_output_shape_for(self, input_shape):
        return self._out_shape</code></pre>
</div>
</div>
<div class="paragraph">
<p>Lets look at this step by step.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code>_ensure_same_size</code> computes smallest \((rows, cols)\) amongst all inputs and uses it to resize all inputs to be the same shape using the provided resize interpolation scheme.</p>
</li>
<li>
<p>We have to define trainable weights in <code>build</code> per keras custom layer <a href="https://keras.io/layers/writing-your-own-keras-layers/">docs</a>. We need as many weights as sum of filters across all inputs to the <code>Connection</code> layer. Weight sharing across all filters of an incoming layer can be achieved by concatenating same weight variable for all filters.</p>
</li>
<li>
<p><code>call</code> computes sigmoid weighted inputs (I tested without sigmoid, and as expected, sigmoid weighing which mostly "allows or disallows inputs" worked a lot better), merged with defined merge strategy. We can tweak <code>init_value</code> and <code>merge_mode</code> to try various init strategies for weights and different merge strategies.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The fully connected net using layers defined below, followed by sequential <code>Dense</code> layers using the above code is shown in fig.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">layers = [
	Convolution2D(32, 3, 3, border_mode='same', activation='relu', bias=False),
	Convolution2D(32, 3, 3, bias=False, activation='relu'),
	MaxPooling2D(pool_size=(2, 2)),
	Dropout(0.25),

	Convolution2D(64, 3, 3, bias=False, activation='relu', border_mode='same'),
	Convolution2D(64, 3, 3, bias=False, activation='relu'),
	MaxPooling2D(pool_size=(2, 2)),
	Dropout(0.25)
]</code></pre>
</div>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/model.png" alt="model">
</div>
<div class="title">Figure 2. Fully connected network from <code>layers</code> followed by sequential <code>Dense</code> layers (open in new tab or download to zoom in).</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_discussion">Discussion</h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Experimentation is still a work in progress. Check back for updates.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Without \(sigmoid\) weighing which mostly "allows or disallows inputs", the convergence was horribly slow. All subsequent results described here used \(sigmoid\) connection weights.</p>
</div>
<div class="sect2">
<h3 id="_experiment1_densenet_style_merge">Experiment1: DenseNet Style merge</h3>
<div class="paragraph">
<p>In these set of experiments, activation maps from prev layers are <em>concatenated</em>.</p>
</div>
<div class="sect3">
<h4 id="_insights_from_initial_exploration">Insights from initial exploration</h4>
<div class="ulist">
<ul>
<li>
<p>Connection weight initialization scheme (init to 0, 1, 0.5) has no effect on convergence.</p>
</li>
<li>
<p>Down sampling interpolation scheme (inter_area, inter_nn, inter_bilinear, inter_bicubic) doesn&#8217;t affect the convergence significantlyfootnote:[inter_bilinear, inter_bicubic work slightly better initially but they all converge to the same final value).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_evolution_of_connection_weights_shared_feature_map_weights">Evolution of connection weights (shared feature map weights)</h4>
<div class="paragraph">
<p>It is definitely interesting to track how connection weights between layers evolved with training epochs. Fig 3 shows the connection weight evolution for connection_o through connection_7 (connection 0 weight 0 corresponds to input&#8594;conv2, connection 0 weight 1 corresponds to conv1&#8594;input2, and so on. Refer to fig 2 to get a complete picture).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/connection_evolution.png" alt="connection_evolution">
</div>
<div class="title">Figure 3. Evolution of various connection weights during training</div>
</div>
<div class="paragraph">
<p>TODO: Discussion.</p>
</div>
</div>
<div class="sect3">
<h4 id="_evolution_of_connection_weights_unique_feature_map_weights">Evolution of connection weights (unique feature map weights)</h4>
<div class="paragraph">
<p>WIP..</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_reproducibility">Reproducibility</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code to reproduce all the experiments is available on <a href="https://github.com/raghakot/deep-learning-experiments/tree/master/exp3">Github</a>. Feel free to reuse or improve.</p>
</div>
<link rel="stylesheet" type="text/css" href="../../../extras/inlineDisqussions.css" />

<script type="text/javascript">
  (function defer() {
    if (window.jQuery) {
      jQuery(document).ready(function() {
          disqus_shortname = 'raghakot-github-io';
          jQuery("p, img").inlineDisqussions();
      });
    } else {
      setTimeout(function() { defer() }, 50);
    }
  })();
</script>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. There are ways to avoid the issue by unrolling the recurrent loops to a fixed number of time steps but I am putting it off for now in the interest of simplicity
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. An excellent overview of LSTMs can be found on <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="bare">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. <a href="https://keras.io/layers/core/#lambda">Lambda layer</a> can be used, but that doesn&#8217;t allow for trainable weights. This is not an issue if tensorflow optimizer was directly used.
</div>
</div>
          </div>
          <footer class="post-footer">
            <div itemprop="author" itemscope itemtype="http://schema.org/Person" class="post-author">
                <a href="http://raghakot.github.io/" class="post-author-avatar">
                  <img itemprop="image" src="https://avatars.githubusercontent.com/u/15642444?v&#x3D;3" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <h4 class="post-footer-heading">Written By</h4>
                <a href="http://raghakot.github.io/" itemprop="url" class="post-author-name">
                  <span itemprop="name">Raghavendra Kotikalapudi</span>
                </a>
                <p itemprop="description" class="post-author-bio">Learning to machine learn</p>
                  <p class="post-author-location">Seattle WA</p>
                  <p class="post-author-website">
                    <a href="http://raghakot.github.io/" rel="nofollow">http://raghakot.github.io/</a>
                  </p>
                <p class="post-info">
                  <b class="post-info-title">Published on</b>
                  <time class="post-date">January 14, 2017</time>
                </p>
              </div>
            </div>
            <div class="post-social">
              <h4 class="post-footer-heading">Spread the word</h4>
              <a href="#" data-action="share-twitter"><i class="fa fa-fw fa-lg fa-twitter"></i></a>
              <a href="#" data-action="share-facebook"><i class="fa fa-fw fa-lg fa-facebook"></i></a>
              <a href="#" data-action="share-gplus"><i class="fa fa-fw fa-lg fa-google-plus"></i></a>
            </div>
          </footer>
        </section>
      <section itemprop="comment" class="post-comments">
        <div id="disqus_thread"></div>
      </section>
    </article>

    <footer role="contentinfo" class="footer">
      <p><small>Copyright &copy; <span itemprop="copyrightHolder">Ragha&#x27;s Blog</span>. 2017. All Rights Reserved.</small></p>
      <p><small><a href="http://ghostium.oswaldoacauan.com/" target="_blank">Ghostium Theme</a> by <a href="http://twitter.com/oswaldoacauan" target="_blank">@oswaldoacauan</a></small></p>
      <p><small>Adapted by <a href="https://twitter.com/mgreau">Maxime Gréau</a></small></p>
      <p><small>Published with <a href="http://hubpress.io">HubPress</a></small></p>
    </footer>
  </div>
</section>


<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  var disqus_shortname = 'raghakot-github-io'; // required: replace example with your forum shortname
  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


          </div>
        </div>
      </div>
    </main>

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//raghakot.github.io/extras/footnotes.js?v="></script> <script src="//raghakot.github.io/extras/inlineDisqussions.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/foot-scripts.min.js?v=1485157443797"></script>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-78195880-1', 'auto');
    ga('send', 'pageview');

    </script>

  </body>
</html>
