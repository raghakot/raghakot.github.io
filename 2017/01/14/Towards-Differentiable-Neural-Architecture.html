<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

    <title>Towards Differentiable Neural Architecture</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Towards Differentiable Neural Architecture">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Towards Differentiable Neural Architecture">
    <meta property="og:description" content="">

    <!-- <meta name="twitter:site" content="">

<meta name="twitter:creator" content="">

<meta name="google-site-verification" content="">

<meta property="fb:admins" content="">
 -->

    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="/apple-touch-icon-precomposed.png" rel="apple-touch-icon">

    <link href="//fonts.googleapis.com/" rel="dns-prefetch">
    <link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Open+Sans:700,400&subset=latin,latin-ext" rel="stylesheet">

    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/main.min.css?v=1508304871325"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/custom.css?v=1508304871325"/>
    <link rel="stylesheet" href="//raghakot.github.io/themes/ghostium/assets/css/asciidoctor-foundation.css?v=1508304871325"/>




    <script type="text/javascript">
      var ga_ua = 'UA-XXXXX-X';
      
      var disqus_shortname = 'example';
      
      var enable_pjax = true;

      // Pace Options
      // ==============
      window.paceOptions = {
        catchupTime: 100,
        minTime: 100,
        elements: false,
        restartOnRequestAfter: 500,
        startOnPageLoad: false
      }

      // Ghostium Globals
      // ==============
      window.GHOSTIUM = {};
      GHOSTIUM.haveGA = typeof ga_ua !== 'undefined' && ga_ua !== 'UA-XXXXX-X';
      GHOSTIUM.haveDisqus = typeof disqus_shortname !== 'undefined' && disqus_shortname !== 'example';
      GHOSTIUM.enablePjax = typeof enable_pjax !== 'undefined' ? enable_pjax : true;
    </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/head-scripts.min.js?v=1508304871325"></script>

    <link rel="canonical" href="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Ragha&#x27;s Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Towards Differentiable Neural Architecture" />
    <meta property="og:description" content="Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture e" />
    <meta property="og:url" content="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    <meta property="article:tag" content="deep learning" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Towards Differentiable Neural Architecture" />
    <meta name="twitter:description" content="Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture e" />
    <meta name="twitter:url" content="https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html" />
    
    <script type="application/ld+json">
null
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="Ragha&#x27;s Blog" href="https://raghakot.github.io/rss/" />
  </head>
  <body class="post-template tag-deep-learning">

    <button data-action="open-drawer" id="drawer-button" class="drawer-button"><i class="fa fa-bars"></i></button>
    <nav tabindex="-1" class="drawer">
      <div class="drawer-container">
        <!--.drawer-search(role="search")-->
        <ul role="navigation" class="drawer-list">
          
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io" data-pjax>
              <i class="fa fa-home"></i>Home
            </a>
          </li>
          <!-- <li class="drawer-list-item">
            <a href="https://raghakot.github.io" title="Ragha&#x27;s Blog" data-pjax>
              <i class="fa fa-list-alt"></i>All posts
            </a>
          </li> -->
          <li class="drawer-list-item">
            <a href="https://raghakot.github.io/rss/">
              <i class="fa fa-rss"></i>Subscribe to Feed
            </a>
          </li>
          <li class="drawer-list-divider"></li>
          <li class="drawer-list-item drawer-list-title">
            Follow me
          </li>
          
          
          <li class="drawer-list-item">
            <a href="https://twitter.com/raghakot" title="Twitter" target="_blank">
              <i class="fa fa-twitter"></i>Twitter
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://github.com/raghakot" title="Github" target="_blank">
              <i class="fa fa-github"></i>Github
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://www.facebook.com/raghavendra.kotikalapudi" title="Facebook" target="_blank">
              <i class="fa fa-facebook"></i>Facebook
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="https://www.linkedin.com/in/raghakot" title="LinkedIn" target="_blank">
              <i class="fa fa-linkedin"></i>LinkedIn
            </a>
          </li>
          <li class="drawer-list-item">
            <a href="mailto:ragha@outlook.com" title="Email" target="_blank">
              <i class="fa fa-envelope-o"></i>Email
            </a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="drawer-overlay"></div>
    <main id="container" role="main" class="container">
      <div class="surface">
        <div class="surface-container">
          <div data-pjax-container class="content">
            
<section class="wrapper wrapper-post">
  <div class="wrapper-container">
    <article itemscope itemtype="http://schema.org/BlogPosting" role="article" class="post post tag-deep-learning">
        <section class="post-container">
          <header class="post-header">
            <ul class="post-meta-list">
              <li class="post-meta-item">
                <time datetime="2017-01-14" itemprop="datePublished">
                  9 months ago
                </time>
              </li>
                <li class="post-meta-item">
                  <span class="tags"><i class="fa fa-tags"></i>
                      <span>
                      <a href="https://raghakot.github.io/tag/deep-learning/">deep learning</a></span>
                  </span>
                </li>
              <li class="post-meta-item">
                <a href="#disqus_thread" data-disqus-identifier="">Comments</a>
              </li>
            </ul>
            <h1 itemprop="name headline" class="post-title"><a href="/" itemprop="url" data-pjax title="Towards Differentiable Neural Architecture">Towards Differentiable Neural Architecture</a></h1>
            <!--h2 itemprop="about" class="post-subtitle"></h2-->
          </header>
          <aside class="post-side">
            <div class="post-author">
                <a href="http://raghakot.github.io/" class="post-author-avatar">
                  <img src="https://avatars2.githubusercontent.com/u/15642444?v&#x3D;4" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <a href="http://raghakot.github.io/" class="post-author-name">
                  Raghavendra Kotikalapudi
                </a>
                <p class="post-author-bio">Learning to machine learn</p>
              </div>
            </div>
          </aside>
          <div itemprop="articleBody" class="post-body">
            <div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Ever since AlexNet was introduced in 2012, neural net research landscape fundamentally changed. With deep computational graphs, the possibilities are endless. We quickly iterated through a dizzying number of architectures such as AlexNet, VGGNet, NIN, Inception, ResNets, FractalNet, Xception, DenseNets and so on. With deep learning, architecture engineering is the new feature engineering and it clearly plays an important role in advancing the state of the art. Instead of making incremental improvements, is there a way to <em>learn</em> the connectivity pattern itself?</p>
</div>
<div class="paragraph">
<p>First step is to make this more concrete. Ideally, we want to learn connectivity amongst individual neurons; instead, lets simplify the problem by constraining ourselves to known layer Lego blocks (by layer Lego blocks, I mean general purpose computational layers such as <em>Convolutional Layer</em>, <em>LSTM</em>, <em>Pooling</em> etc.). Given a bunch of Layer Legos \(L = \{L_{1}, L_{2}, \cdots, L_{n}\}\), our task is to learn how these should be assembled. First thing that comes to mind is evolutionary search, but that is computationally prohibitive. A quicker and more elegant alternative would be to learn this by computing the gradient of <em>architecture</em> with respect to loss. The challenge is to make <em>architecture</em> itself differentiable.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/legos.jpg" alt="legos" width="400">
</div>
<div class="title">Figure 1. Deep learning practitioners</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_differentiable_architecture">Differentiable Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Architecture can mean a lot of things. For now, let us constrain ourselves to connectivity patterns. Given some \(n\) layers, what is the optimal connectivity amongst them? The naive way of approaching this problem would be to try out all \(n(n-1)\) possible connection configurations<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnote_1" title="View footnote.">1</a>]</sup>. There is a resnet, densenet or some other future net hidden in there.</p>
</div>
<div class="paragraph">
<p>Bruteforce search is not such a bad option. Cifar10/100 networks are quite short (relatively speaking). If you have access to huge compute, it might take a month to try out all possible connectivities. Hopefully we can extrapolate the recurring blocks and apply it to imagenet and get a new SOTA results (wishful thinking). While practical, this is a rather boring way to solve the problem. What if we can learn the connections via gradient descent? For this to work, we need a way to compute \(\frac{\partial (loss)}{\partial (connection)}\).</p>
</div>
<div class="paragraph">
<p>Connectivity is inherently binary. This is problematic for applying gradient descent because of the discontinuity. Notice the similarities with attention? We can employ reinforcement learning like hard attention or try soft attention strategy. In order to make connectivity <em>continuous</em>, we can make it weighted, similar to <em>forget gate</em> in LSTMs<sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnote_2" title="View footnote.">2</a>]</sup>. To bias the connectivity mostly towards 0 or 1, we can apply use sigmoid weighted connectivity. To recap, the strategy is to start out with a fully connected net where evry layer is connected to every other layer via sigmoid weighted param. The weights can be learned via backprop.</p>
</div>
<div class="paragraph">
<p>Cool. There is one slight technicality. Backward connections introduce cycles in the graph<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnote_3" title="View footnote.">3</a>]</sup>. Excluding backward connections, every layer can have incoming inputs from all previous layers, which leaves us with \(\frac{n(n-1)}{2}\) possible connections. This is exactly what <a href="https://arxiv.org/pdf/1608.06993v3.pdf">DenseNets</a> do!, except that we arrived at a similar architecture with a different motivation.</p>
</div>
<div class="paragraph">
<p>Unlike DenseNets, we want the connections to be gated so that they can be pruned if needed. There are a couple of possibilities for how \(L_{i}\) is connected to \(L_{j}\).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Shared/Unique feature weights</strong>: We can either associate weights \(W = [w_{1}, w_{2}, \cdots, w_{f}]\) for \(L_{i}\) with \(f\) feature maps or use a single weight for all feature maps of an incoming input. Lets call this <em>shared</em>/<em>unique</em> feature weight scheme.</p>
</li>
<li>
<p><strong>Input aggregation</strong>: For layer \(L_{j}\), we need a way to aggregate weight feature maps from previous layers. Some possibilities include:</p>
<div class="ulist">
<ul>
<li>
<p><strong>DenseNet style concat</strong>: i.e, simply concatenate previous layer weighted feature maps. This has the limitation that concatenation is not viable when the size of feature maps differ. For this we can either try to pad smaller feature maps
with 0&#8217;s to make them all the same size as the largest feature map or resize all feature maps to the smallest feature map by pooling/interpolation/some other form of down sampling. Both approaches have their own pros and cons. The first is computationally expensive (since convnets have to slide over larger feature maps), while the second one is lossy. I have a feeling that lossy would end up being the winner.</p>
</li>
<li>
<p><strong>ResNet style concat</strong>: i.e., squish weighted input feature maps via <em>max</em> or <em>sum</em>. <em>avg</em>, <em>prod</em>, <em>min</em> dont seem like good ideas for obvious reasons. I like <em>max</em> better than resnet style <em>sum</em>, since it is equivalent to focusing on input patterns that matched with a filter (high output value) across various feature maps. A high value at some \((row, col)\) of the filter might indicate the presence of some abstract shape detected by previous convolutional layers.</p>
</li>
<li>
<p><strong>Squeeze net style concat</strong>: Lastly, we can also try squeeze net strategy of squishing \(n\) concatenated input feature maps (after reshaping to same size via padding or down sampling) down to \(m\) feature maps where \(m \le n\).</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Phew! That completes our setup. The idea is super simple. Start out with an fully connected network where every layer gets inputs from all the previous layers in a weighted fashion. Before weighing, we use \(sigmoid(W)\) to bias the values mostly towards 0 or 1. When trained end-end via backprop, the network should, in-principle, learn to prune unwanted connections.</p>
</div>
<div class="paragraph">
<p>We can also try doing couple of other things.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Take the converged connectivity pattern and try it exclusively by removing connections with small weights. If \(w = 0.002\) or something like that, we know that the network is trying really hard to get rid of that connection. So might as well see if the performance is better when we start out by removing it.</p>
</li>
<li>
<p>Examine the evolution of weights during training. Do they always converge in the same manner? If they don&#8217;t, that would be concerning.</p>
</li>
<li>
<p>Try different connection weight initialization schemes. \(w = 1\) would mean everything starts out fully connected. Alternatively we can set \(w\) from a Gaussian distribution centered around 0.5. We can try other funky things like setting initial feed forward stack \(L_{i-1}\) &#8594; \(L_{i}\) weight to 1 and rest to 0.5.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_experimental_setup">Experimental Setup</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Layers = [Conv, Conv, MaxPool, Dropout] \(\times\) 2-3 blocks of variable feature sizes.</p>
</li>
<li>
<p>cifar10 dataset augmented with 10% random shifts along image rows/cols along with a 50% chance of horizontal flip.</p>
</li>
<li>
<p><code>random_seed = 1337</code> for reproducibility.</p>
</li>
<li>
<p>Training for 200 epochs, batch size of 32 using categorical cross-entropy loss with Adam optimizer.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_implementation">Implementation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A quick summary of these ideas are translated into concrete implementation. A complete implementation can be found on my <a href="https://github.com/raghakot/deep-learning-experiments/tree/master/exp3">github</a>.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Creating fully connected net</strong>. i.e, a layer is connected to all prev layer outputs. This is not as hard as it appears.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">def make_fully_connected(x, layers):
    inputs = [x]
    for layer in layers:
        x = layer(x)
        inputs.append(x)
        # This is the part where we resize inputs to be of same shape and merge them in resnet or densenet style
    return x</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Merging</strong>. i.e., resizing prev layer outputs to be of the same shape and concatenating them in densenet or resnet style. We also want to weigh merged outputs so that those weights can be learned during backprop. The easiest way to do this in keras is to create a custom layer<sup class="footnote">[<a id="_footnoteref_4" class="footnote" href="#_footnote_4" title="View footnote.">4</a>]</sup>.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">import numpy as np
import tensorflow as tf

from keras import backend as K
from keras.layers import Lambda, Layer

class Connection(Layer):
    """Takes a list of inputs, resizes them to the same shape, and outputs a weighted merge.
    """
    def __init__(self, init_value=0.5, merge_mode='concat',
                 resize_interpolation=tf.image.ResizeMethod.BILINEAR,
                 shared_weights=True):
        """Creates a connection that encapsulates weighted connection of input feature maps.

        :param init_value: The init value for connection weight
        :param merge_mode: Defines how feature maps from inputs are aggregated.
        :param resize_interpolation: The downscaling interpolation to use. Instance of `tf.image.ResizeMethod`.
            Note that ResizeMethod.AREA will fail as its gradient is not yet implemented.
        :param shared_weights: True to share the same weight for all feature maps from inputs[i].
        False creates a separate weight per feature map.
        """
        self.init_value = init_value
        self.merge_mode = merge_mode
        self.resize_interpolation = resize_interpolation
        self.shared_weights = shared_weights
        super(Connection, self).__init__()

    def _ensure_same_size(self, inputs):
        """Ensures that all inputs match last input size.
        """
        # Find min (row, col) value and resize all inputs to that value.
        rows = min([K.int_shape(x)[1] for x in inputs])
        cols = min([K.int_shape(x)[2] for x in inputs])
        return [tf.image.resize_images(x, [rows, cols], self.resize_interpolation) for x in inputs]

    def _merge(self, inputs):
        """Define other merge ops like [+, X, Avg] here.
        """
        if self.merge_mode == 'concat':
            # inputs are already stacked.
            return inputs
        else:
            raise RuntimeError('mode {} is invalid'.format(self.merge_mode))

    def build(self, input_shape):
        """ Create trainable weights for this connection
        """
        # Number of trainable weights = sum of all filters in `input_shape`
        nb_filters = sum([s[3] for s in input_shape])

        # Create shared weights for all filters within an input layer.
        if self.shared_weights:
            weights = []
            for shape in input_shape:
                # Create shared weight, make nb_filter number of clones.
                shared_w = K.variable(self.init_value)
                for _ in range(shape[3]):
                    weights.append(shared_w)
            self.W = K.concatenate(weights, axis=-1)
        else:
            self.W = K.variable(np.ones(shape=nb_filters) * self.init_value)

        self._trainable_weights.append(self.W)
        super(Connection, self).build(input_shape)

    def call(self, layer_inputs, mask=None):
        # Resize all inputs to same size.
        resized_inputs = self._ensure_same_size(layer_inputs)

        # Compute sigmoid weighted inputs
        stacked = K.concatenate(resized_inputs, axis=-1)
        weighted = stacked * K.sigmoid(self.W)

        # Merge according to provided merge strategy.
        merged = self._merge(weighted)

        # Cache this for use in `get_output_shape_for`
        self._out_shape = K.int_shape(merged)
        return merged

    def get_output_shape_for(self, input_shape):
        return self._out_shape</code></pre>
</div>
</div>
<div class="paragraph">
<p>Lets look at this step by step.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code>_ensure_same_size</code> computes smallest \((rows, cols)\) amongst all inputs and uses it to resize all inputs to be the same shape using the provided resize interpolation scheme.</p>
</li>
<li>
<p>We have to define trainable weights in <code>build</code> per keras custom layer <a href="https://keras.io/layers/writing-your-own-keras-layers/">docs</a>. We need as many weights as sum of filters across all inputs to the <code>Connection</code> layer. Weight sharing across all filters of an incoming layer can be achieved by concatenating same weight variable for all filters.</p>
</li>
<li>
<p><code>call</code> computes sigmoid weighted inputs (I tested without sigmoid, and as expected, sigmoid weighing which mostly "allows or disallows inputs" worked a lot better), merged with defined merge strategy. We can tweak <code>init_value</code> and <code>merge_mode</code> to try various init strategies for weights and different merge strategies.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The fully connected net using layers defined below, followed by sequential <code>Dense</code> layers using the above code is shown in fig.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-python" data-lang="python">layers = [
	Convolution2D(32, 3, 3, border_mode='same', activation='relu', bias=False),
	Convolution2D(32, 3, 3, bias=False, activation='relu'),
	MaxPooling2D(pool_size=(2, 2)),
	Dropout(0.25),

	Convolution2D(64, 3, 3, bias=False, activation='relu', border_mode='same'),
	Convolution2D(64, 3, 3, bias=False, activation='relu'),
	MaxPooling2D(pool_size=(2, 2)),
	Dropout(0.25)
]</code></pre>
</div>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/model.png" alt="model">
</div>
<div class="title">Figure 2. Fully connected network from <code>layers</code> followed by sequential <code>Dense</code> layers (open in new tab or download to zoom in).</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_discussion">Discussion</h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Experimentation is still a work in progress. Check back for updates.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Without \(sigmoid\) weighing which mostly "allows or disallows inputs", the convergence was horribly slow. All subsequent results described here used \(sigmoid\) connection weights.</p>
</div>
<div class="sect2">
<h3 id="_experiment1_densenet_style_merge">Experiment1: DenseNet Style merge</h3>
<div class="paragraph">
<p>In these set of experiments, activation maps from prev layers are <em>concatenated</em>.</p>
</div>
<div class="sect3">
<h4 id="_insights_from_initial_exploration">Insights from initial exploration</h4>
<div class="ulist">
<ul>
<li>
<p>Connection weight initialization scheme (init to 0, 1, 0.5) has no effect on convergence.</p>
</li>
<li>
<p>Down sampling interpolation scheme (inter_area, inter_nn, inter_bilinear, inter_bicubic) doesn&#8217;t affect the convergence significantly<sup class="footnote">[<a id="_footnoteref_5" class="footnote" href="#_footnote_5" title="View footnote.">5</a>]</sup>.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_evolution_of_connection_weights_shared_feature_map_weights">Evolution of connection weights (shared feature map weights)</h4>
<div class="paragraph">
<p>It is definitely interesting to track how connection weights between layers evolved with training epochs. Fig 3 shows the connection weight evolution for connection_o through connection_7 (connection 0 weight 0 corresponds to input&#8594;conv2, connection 0 weight 1 corresponds to conv1&#8594;input2, and so on. Refer to fig 2 to get a complete picture).</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="https://raghakot.github.io/images/diff_neural/connection_evolution.png" alt="connection_evolution">
</div>
<div class="title">Figure 3. Evolution of various connection weights during training</div>
</div>
<div class="paragraph">
<p>TODO: Discussion.</p>
</div>
</div>
<div class="sect3">
<h4 id="_evolution_of_connection_weights_unique_feature_map_weights">Evolution of connection weights (unique feature map weights)</h4>
<div class="paragraph">
<p>WIP..</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_reproducibility">Reproducibility</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The code to reproduce all the experiments is available on <a href="https://github.com/raghakot/deep-learning-experiments/tree/master/exp3">Github</a>. Feel free to reuse or improve.</p>
</div>
<link rel="stylesheet" type="text/css" href="../../../extras/inlineDisqussions.css" />

<script type="text/javascript">
  (function defer() {
    if (window.jQuery) {
      jQuery(document).ready(function() {
          disqus_shortname = 'raghakot-github-io';
          jQuery("p, img").inlineDisqussions();
      });
    } else {
      setTimeout(function() { defer() }, 50);
    }
  })();
</script>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnote_1">
<a href="#_footnoteref_1">1</a>. Don&#8217;t give such answers in interviews, lol
</div>
<div class="footnote" id="_footnote_2">
<a href="#_footnoteref_2">2</a>. An excellent overview of LSTMs can be found on <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" class="bare">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
</div>
<div class="footnote" id="_footnote_3">
<a href="#_footnoteref_3">3</a>. There are ways to avoid the issue by unrolling the recurrent loops to a fixed number of time steps but lets put that off for now in the interest of simplicity
</div>
<div class="footnote" id="_footnote_4">
<a href="#_footnoteref_4">4</a>. <a href="https://keras.io/layers/core/#lambda">Lambda layer</a> can be used, but that doesn&#8217;t allow for trainable weights. This is not an issue if tensorflow optimizer was directly used.
</div>
<div class="footnote" id="_footnote_5">
<a href="#_footnoteref_5">5</a>. inter_bilinear, inter_bicubic work slightly better initially but they all converge to the same final value
</div>
</div>
          </div>
          <footer class="post-footer">
            <div itemprop="author" itemscope itemtype="http://schema.org/Person" class="post-author">
                <a href="http://raghakot.github.io/" class="post-author-avatar">
                  <img itemprop="image" src="https://avatars2.githubusercontent.com/u/15642444?v&#x3D;4" alt="Raghavendra Kotikalapudi">
                </a>
              <div class="post-author-info">
                <h4 class="post-footer-heading">Written By</h4>
                <a href="http://raghakot.github.io/" itemprop="url" class="post-author-name">
                  <span itemprop="name">Raghavendra Kotikalapudi</span>
                </a>
                <p itemprop="description" class="post-author-bio">Learning to machine learn</p>
                  <p class="post-author-location">Seattle WA</p>
                  <p class="post-author-website">
                    <a href="http://raghakot.github.io/" rel="nofollow">http://raghakot.github.io/</a>
                  </p>
                <p class="post-info">
                  <b class="post-info-title">Published on</b>
                  <time class="post-date">January 14, 2017</time>
                </p>
              </div>
            </div>
            <div class="post-social">
              <h4 class="post-footer-heading">Spread the word</h4>
              <a href="#" data-action="share-twitter"><i class="fa fa-fw fa-lg fa-twitter"></i></a>
              <a href="#" data-action="share-facebook"><i class="fa fa-fw fa-lg fa-facebook"></i></a>
              <a href="#" data-action="share-gplus"><i class="fa fa-fw fa-lg fa-google-plus"></i></a>
            </div>
          </footer>
        </section>
      <section itemprop="comment" class="post-comments">
        <div id="disqus_thread"></div>
      </section>
    </article>

    <footer role="contentinfo" class="footer">
      <p><small>Copyright &copy; <span itemprop="copyrightHolder">Ragha&#x27;s Blog</span>. 2017. All Rights Reserved.</small></p>
      <p><small><a href="http://ghostium.oswaldoacauan.com/" target="_blank">Ghostium Theme</a> by <a href="http://twitter.com/oswaldoacauan" target="_blank">@oswaldoacauan</a></small></p>
      <p><small>Adapted by <a href="https://twitter.com/mgreau">Maxime Gréau</a></small></p>
      <p><small>Published with <a href="http://hubpress.io">HubPress</a></small></p>
    </footer>
  </div>
</section>


<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
  var disqus_shortname = 'raghakot-github-io'; // required: replace example with your forum shortname
  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


          </div>
        </div>
      </div>
    </main>

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script> <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//raghakot.github.io/extras/footnotes.js?v="></script> <script src="//raghakot.github.io/extras/inlineDisqussions.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();
      </script>

    <script src="//raghakot.github.io/themes/ghostium/assets/js/foot-scripts.min.js?v=1508304871325"></script>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-78195880-1', 'auto');
    ga('send', 'pageview');

    </script>

  </body>
</html>
