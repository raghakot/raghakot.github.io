= Learning good image representation
:hp-tags: deep learning

This is a quick blog about an idea that occurred to me yesterday. I am currently working on experiments involving link:https://raghakot.github.io/2017/01/14/Towards-Differentiable-Neural-Architecture.html[differentiable neural architecture], but the idea got me excited enough to write about it. I will follow up and update this post with experiments as soon as possible.

We currently represent an image as \((r, g, b)\) pixel values. When trained end-end, each layer transforms image into representations that are useful for the task at hand. Whenever we come up with a new architecture/task, these representations are learned again from scratch. Intuitively, all image classification problems, be it imagenet or flowers dataset should share common intermediate representations. If we could learn an image representation that works for all image problems, it would go a long way in making some progress towards _one shot learning_.

= Learning image representation

I really liked the idea of using an encoder-decoder architecture for learning useful image representations. An encoder is a neural network that maps input image to some latent image representation (\(Z\)), i.e., \(encoder: \Bbb R^{rows \times cols \times channels} \rightarrow \Bbb R^d\). Decoder (another neural network) is tasked to reconstruct the original input image from the latent representation \(Z\). The key idea is to set \(d\) to be a lot smaller than \(rows \times cols \times channels\) so that the encoder is forced to discover and efficiently internalize the essence of the data as a compressed representation while being useful enough for reconstruction.

[.text-center]
.Encoder-Decoder Architecture
image::repr_learning/encoder-decoder.png[encoder-decoder, 500]

Then came supervised conv nets, beating all sorts of records on imagenet. This is because learning a "useful representation" depends on the task. For example, a water classifier might care about learning representations that make it easy to detect blue blobs, while an animal classifier might learn to detect shapes. Of late, most unsupervised learning focus has shifted towards Generative Advererial Networksfootnote:[An excellent overview of GANs can be found at https://openai.com/blog/generative-models/] as they showed very promising results in modeling probability distributions of complex real world data.

I feel that we combine all three components.

* Discrimination: We obviously know how to recognize and learn new objects.
* Generative aspect: When we look at a new object, say a bottle, we can imagine possible variations and how it would look if we rotate them even though its completely new. Like generative networks, the image is not perfect when we imagine it in ur heads.
* Latent representation: In the above example, I suspect that brain is maps both apple and bottle as an entity that can be rotated (latent representation), which might be aiding generative and discriminative aspects.

An encoder-decoder also possess generative abilities provided that perturbations in latent representation decode into useful object variations. Instead of using the standard encoder-decoder pipeline, what if we condition additional constraints on Z? For example, we could condition Z such that the decoder can decode the image and a VGGNet learns to classify with Z as input. We could also use an additional network to learn classification. The idea is to constrain Z so that it is useful in all three applications. 



++++
<link rel="stylesheet" type="text/css" href="../../../extras/inlineDisqussions.css" />

<script type="text/javascript"> 
  (function defer() {
    if (window.jQuery) {      
      jQuery(document).ready(function() {      	
          disqus_shortname = 'raghakot-github-io';
          jQuery("p, img").inlineDisqussions();        
      });
    } else {
      setTimeout(function() { defer() }, 50);     
    }
  })(); 
</script>
++++