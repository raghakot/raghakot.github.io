= SimNeuron - A simplified neural computational model.

== Overview

Its 2017, I assume everyone is familiar with deep learning and the waves conv-nets have been making with image, text and speech applications. Despite different variations of neural networks such as LSTMs, ConvNets, Standard neural nets, endcoder-decoder architectures and so on, the computational model of a neuron hasn't changed since inception. Every neuron has \(n\) incomming inputs \(X = (x_{1}, x_{2}, ..., x_{n})\) with weights \(W = (w_{1}, w_{2}, ..., w_{n})\). A neuron then computes \(W.X = \sum_{i=0}^{n}w_{i}.x_{i}\) and then uses some nonlinearity like sigmoid or ReLU to generate the output. All architectures build on this fundamental computational model. Let us examine what the computation is conceptually doing in a step by step fashion.

1. Each input \(x_{i}\) is being magnified or diminished by multiplying by a scaling factor \(w_{i}\). This can be interpreted as enhancing or diminishing the contribution of some pattern component \(x_{i}\) to the output decision. Think of these weights as _knobs_ that range from \([-\inf, +\inf]\). The task of learning is to _tune_ these knobs to the correct value.
* If we consider the computation \(W.X\), 