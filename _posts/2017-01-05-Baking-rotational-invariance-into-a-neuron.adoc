= Baking rotational invariance into a neuron
:hp-tags: deep learning

This is a continuation from the first link:https://raghakot.github.io/2017/01/03/Exploring-alternative-neural-computational-models.html[post] on exporing alternative neural computational models. To recap quickly, a neuron is fundamentally acting as a pattern matching unit by computing the dot product \(W \cdot X\), which is equivalent to un-normalized cosine similarity between vectors \(W\) and \(X\).

This model of pattern matching is very naive, in a sense that it is not invariant to translations or rotations. Consider the following weight vector template (conv filter) corresponding to vertical edge pattern `|`:

\(W = \begin{bmatrix}
0.2 & 1.0 & 0.2\\ 
1.0 & 1.0 & 1.0\\
0.2 & 1.0 & 0.2 
\end{bmatrix} \)

[.text-center]
.Visual representation of \(W\) template
image::alt_neural2/vertical_line_filter.png[vertical_line_filter, 300]
{empty} +

When a neuron tries to match this pattern in a \(3 \times 3\) input image patch, it will only output a high value if a vertical edge is found in the center of the image patch. i.e., dot product is not invariant to shifts. My first thought was to use link:https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient[pearson coefficient] as it provides shift invariance. However, this not a problem in practice as the filter is slid across all possible \(3 \times 3\) patches within the input image (see fig2).

[.text-center]
.Sliding filter over input image to generate the output activation area (image borrowed from link:http://intellabs.github.io/RiverTrail/tutorial/[river trail documentation])
image::alt_neural2/conv_illustration.png[conv_illustration, 400]
{empty} +

In a way, the sliding window operation is quite clever as it communicates information about _parts of the image_ matching filter template without increasing the number of _trainable_ parameters.  As a concrete example, consider a conv-net with filters as shown in fig3footnote:[First layer filters generally detect basic lines and textures, this is just a simplified view to illustrate the point]. The first layer filters (green) detect eyes, nose etc., second layer filters (yellow) detect face, leg etc., by aggregating features from first layer and so onfootnote:[In reality, convolution filters may detect objects that have no meaningful interpretation].

[.text-center]
.Hypothetical conv-net filters to illustrate how a `human` might be detected. (image borrowed from quora link:https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features[post])
image::alt_neural2/conv_net1.png[conv_illustration, 600]
{empty} +

Now suppose the face (represented by two red and one magenta point) is moved to another corner as shown in fig4. The same activations occur, leading to the same end result.

[.text-center]
.Illustration of activations when an input object is shifted. (image borrowed from quora link:https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features[post])
image::alt_neural2/conv_net2.png[conv_illustration, 600]
{empty} +

So far so good. How about rotational invariance? We know that a lot of learned conv filters are identical, but rotated by some non-random factor. Instead of learning rotational invariance in this manner, we will try to build it directly into the computational model. The motivativation is simple - the neuron should output high value even if some rotated version of pattern is present in the input. Hopefully this eliminates redundant filters and improves test time accuracy.

[.text-center]
.A few conv filters of a vggnet (image borrowed from keras link:https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html[blog post]). Notice how some filters are identical but rotated.
image::alt_neural2/conv_filters.jpg[conv_filters, 600]
{empty} +

== Endowing rotational invariance

How can we build rotational invariance into dot product similarity computation? After searching for a bit on the internet, I couldn't find any obvious similarity metrics that do this. Sure there is link:https://en.wikipedia.org/wiki/Scale-invariant_feature_transform[SIFT] and link:https://en.wikipedia.org/wiki/Speeded_up_robust_features[SURF], but they are complex computations and cannot be expressed in terms of dot products alone.

Instead of trying to come up with a metric, I decided to try the brute force approach of matching the input patch with all possible rotations of the filter. If we take `max` of all those outputs, then, in principle, we are choosing the output resulting from the rotated filter that best `aligns` with the pattern in input patch. This startegy is partly inspired by how conv nets gets around translational invariance problem by simply sliding the filter over all possible locations. Unlike sliding operation, which is an _architectural_ property rather than the computational property of the neuron, the brute force rotations of filters can be confined within the abstraction of neuron, making it a part of the computational model.

Lets look at this idea in more concrete terms. A neuron receives input \(X\) and has associated weights \(W\). Let \(r\) denote some arbitrary discrete rotation \(\theta\). Instead of directly computing \(W \cdot X\), we will compute \(\max\left \{ rW \cdot X, r^{2}W \cdot X \right, \cdots, r^{k}W \cdot X \}\), where \(k = \left \lceil \frac{360}{\theta} \right \rceil\) is the number of brute force rotations of \(\theta\) step.

This idea has a number of practical challenges.

* How do we decide \(\theta\)? If we set \(\theta = 90^{\circ}\), we can produce 4 rotated filters.
* For \(\theta \neq 90^{\circ}\) we need interpolation, which might change filter shape.

To avoid interpolation, we can try to shift the image by rotating elements clockwise from outermost layer to innermost layer. For a \(3 \times 3\) image, there are 8 possible rotations. Unfortunately, this only works for filters that are symmetric along the center of the matrix. As an example, consider the filter shaped as `L`

[.text-center]
.A few conv filters of a vggnet (image borrowed from keras link:https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html[blog post]). Notice how some filters are identical but rotated.
image::alt_neural2/rotations_L.jpg[rotations_of_filter, 600]
{empty} +





== Future work
Lets look at it in another way; We only care about computing the dot product with the `best` aligned filter. Perhaps we can try to find \(\theta\) that _maximizes_ the dot product using calculus.

Let R denote \(2 \times \2) rotation matrix.

\(R = \begin{bmatrix}
cos(\theta) & sin(\theta)\\ 
-sin(\theta) & cos(\theta) 
\end{bmatrix}\)

To accomplish optimal alignment, we need \(\theta\) such that:

\(\arg\max_\theta R_{\theta}W \cdot X\)

This can be solved by differentiating with respect to \(\theta\) and equating the derivative to 0.


